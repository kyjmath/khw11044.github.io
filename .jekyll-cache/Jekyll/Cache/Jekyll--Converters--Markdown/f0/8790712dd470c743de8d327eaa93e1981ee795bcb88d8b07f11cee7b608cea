I"ö<p>ë”¥ëŸ¬ë‹ ìš©ì–´ ì •ë¦¬</p>

<p>ë”¥ëŸ¬ë‹ ìš©ì–´ ì‚¬ì „</p>

<p>DeepLearning</p>

<p>ë…¸ë“œ</p>

<p>ê°€ì¤‘ì¹˜</p>

<p>ë°”ì´ì–´ìŠ¤</p>

<p>í™œì„±í™” í•¨ìˆ˜</p>

<p>ReLU</p>

<p>LeakyReLU</p>

<p>PReLU</p>

<p>MaxOut</p>

<p>sigmoid</p>

<p>softmax</p>

<p>ì„ í˜• íšŒê·€</p>

<p>ë¡œì§€ìŠ¤í‹± íšŒê·€</p>

<p>ìŠ¤í”„íŠ¸ë§¥ìŠ¤ íšŒê·€</p>

<p>ë¶„ë¥˜</p>

<p>í¸ì°¨(Deviation)</p>

<p>ì”ì°¨</p>

<p>ì˜¤ì°¨</p>

<p>Gradient Descent</p>

<p>Adam</p>

<p>Momentum</p>

<p>RMSProp</p>

<p>Vanshing Gradient</p>

<p>Overfitting</p>

<p>Blackbox</p>

<p>end-to-end</p>

<p>Forward Propagation</p>

<p>Backward Propagation</p>

<p>Perceptron</p>

<p>Dropout</p>

<p>Fine Tuning</p>

<p>pruning</p>

<p>Transfer Learning</p>

<p>GAN</p>

<p>DeepLearning Quantization</p>

<p>meta learning</p>

<p>matching network</p>

<p>vot</p>

<p>object detection</p>

<p>RBM(Restricted Boltzmann Machine)</p>

<p>ì› í•« ì¸ì½”ë”©</p>

<p>í‰ê·  ì œê³± ì˜¤ì°¨ (Mean squared error: MSE)</p>

<p>í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ í•¨ìˆ˜(Cross-entropy function)</p>

<p>Loss Function</p>

<p>Cost Function</p>

<p>ìì½”ë¹„ì•ˆ</p>

<p>í•©ì„±ê³± ì‹ ê²½ë§(Convolutional neural network)</p>

<p>ì±„ë„(Channel)</p>

<p>íŒ¨ë”©(Padding)</p>

<p>í’€ë§(Pooling)</p>

<p>ìŠ¤íŠ¸ë¼ì´ë“œ(Stride)</p>

<p>ìˆœí™˜ ì‹ ê²½ë§(Recurrent neural network)</p>

<p>ìƒì‚°ì  ì ëŒ€ ì‹ ê²½ë§(Generative adversarial network)</p>

<p>ê·¸ë˜í”„ ì‹ ê²½ë§(Graph neural network)</p>

<p>ì§€ë„í•™ìŠµ</p>

<p>ë¹„ì§€ë„í•™ìŠµ</p>

<p>ì¤€ì§€ë„í•™ìŠµ</p>

<p>ê°•í™”í•™ìŠµ</p>

<p>Chain Rule(ì—°ì‡„ë²•ì¹™)</p>

<p>(Explainable artificial intelligence : EAI)</p>

<p>AutoEncoder</p>

<p>ì •ê·œí™”</p>

<p>Regularization</p>

<p>L1 Regularization</p>

<p>L2 Regularization</p>

<p>Normalization</p>

<p>scaling</p>

<p>L1 Norm</p>

<p>L2 Norm</p>
:ET